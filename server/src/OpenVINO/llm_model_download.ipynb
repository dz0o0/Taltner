{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npoetry add wheel\\n\\npoetry add --group dev \\'ipykernel\\' \\'ipywidgets\\'\\n\\npoetry add --group llm \\'openvino\\'    \"git+https://github.com/huggingface/optimum-intel.git\"    \"git+https://github.com/openvinotoolkit/nncf.git\"\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 依存関係\n",
    "'''\n",
    "poetry add wheel\n",
    "\n",
    "poetry add --group dev 'ipykernel' 'ipywidgets'\n",
    "\n",
    "poetry add --group llm 'openvino'\\\n",
    "    \"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "    \"git+https://github.com/openvinotoolkit/nncf.git\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from llm_config import LLM_MODELS_CONFIG\n",
    "\n",
    "import openvino as ov\n",
    "import nncf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modelがgemmaの場合は、ログインが必要\n",
    "def login_huggingface_hub():\n",
    "    from huggingface_hub import notebook_login, whoami\n",
    "    try:\n",
    "        whoami()\n",
    "        print('Authorization token already provided')\n",
    "    except OSError:\n",
    "        notebook_login()\n",
    "        print('Authorization token has been provided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nncf.set_log_level(logging.ERROR)\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "# デバイスの特定\n",
    "support_devices = core.available_devices\n",
    "device = 'NPU' if 'NPU' in support_devices else 'CPU'\n",
    "\n",
    "\n",
    "# ダウンロードするモデル\n",
    "# NPUならPhi-3-mini-4k-instruct, CPUならgemma-2b-it\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\" if device == 'NPU' else \"google/gemma-2b-it\"\n",
    "\n",
    "## modelがgemmaの場合は、HuggingFaceのログインが必要\n",
    "if model_id == \"google/gemma-2b-it\":\n",
    "    login_huggingface_hub()\n",
    "\n",
    "model_name = model_id.split('/')[-1]\n",
    "\n",
    "remote_code = LLM_MODELS_CONFIG[model_name]['remote_code']  # Phi-3はTrue\n",
    "\n",
    "# モデルを保存するディレクトリ\n",
    "model_dir = Path(f'../../model/{model_name}')\n",
    "fp16_model_dir = model_dir / \"FP16\"  # float 16bitモデルの保存先\n",
    "int8_model_dir = model_dir / \"INT8\"  # 量子化モデルの保存先(8bit)\n",
    "int4_model_dir = model_dir / \"INT4\"  # 量子化モデルの保存先(4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NPUの要件がINT8以上なので、コメントアウト\n",
    "\n",
    "# INT4の設定\n",
    "compression_configs = {\n",
    "    # ここのパラメータは要調整\n",
    "    # \"sym\":          対称量子化の利用\n",
    "    # 'group_size':  グループサイズ  (64, 128が無難？)\n",
    "    # 'ratio':       量子化後のパラメータの割合  (0.5~0.8で試す)\n",
    "    \"gemma-2b-it\": {\n",
    "        \"sym\": True,\n",
    "        \"group_size\": 64,\n",
    "        \"ratio\": 0.6,\n",
    "    },\n",
    "    \"default\": {\n",
    "        \"sym\": False,\n",
    "        \"group_size\": 128,\n",
    "        \"ratio\": 0.8,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google/gemma-2b-it'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimum-cliでモデルをopenvino形式でダウンロード\n",
    "export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past\".format(model_id)\n",
    "\n",
    "def convert_to_fp16():\n",
    "    global export_command_base\n",
    "    export_command = ''\n",
    "    # すでに存在する場合はスキップ\n",
    "    if (fp16_model_dir / \"openvino_model.xml\" ).exists():\n",
    "        return\n",
    "    if remote_code:\n",
    "        # Phi-3のみ\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" --weight-format fp16\"\n",
    "    export_command += \" \" + str(fp16_model_dir)\n",
    "    # モデルのダウンロード開始時間\n",
    "    start_model_download = datetime.now()\n",
    "    print('export_command:', export_command)\n",
    "    os.system(export_command)  # モデルのダウンロード\n",
    "    # モデルのダウンロード終了時間\n",
    "    end_model_download = datetime.now() - start_model_download\n",
    "    print('export done', end_model_download.total_seconds())\n",
    "\n",
    "\n",
    "def convert_to_int8():\n",
    "    global export_command_base\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" --weight-format int8\"\n",
    "    export_command += \" \" + str(int8_model_dir)\n",
    "    # モデルのダウンロード開始時間\n",
    "    start_model_download = datetime.now()\n",
    "    print('export_command:', export_command)\n",
    "    os.system(export_command)  # モデルのダウンロード\n",
    "    # モデルのダウンロード終了時間\n",
    "    end_model_download = datetime.now() - start_model_download\n",
    "    print('export done', end_model_download.total_seconds())\n",
    "\n",
    "\n",
    "# NPUの要件がINT8以上なので、コメントアウト\n",
    "def convert_to_int4():\n",
    "    global export_command_base\n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    # 量子化の設定\n",
    "    model_compression_params  = compression_configs.get(model_name, compression_configs[\"default\"])\n",
    "    export_command = export_command_base + \" --weight-format int4\"\n",
    "    int4_compression_args = \" --group-size {} --ratio {}\".format(model_compression_params[\"group_size\"], model_compression_params[\"ratio\"])\n",
    "    if model_compression_params[\"sym\"]:\n",
    "        int4_compression_args += \" --sym\"\n",
    "    export_command += int4_compression_args + \" \" + str(int4_model_dir)\n",
    "    # モデルのダウンロード開始時間\n",
    "    start_model_download = datetime.now()\n",
    "    print('export_command:', export_command)\n",
    "    os.system(export_command)  # モデルのダウンロード\n",
    "    # モデルのダウンロード終了時間\n",
    "    end_model_download = datetime.now() - start_model_download\n",
    "    print('export done', end_model_download.total_seconds())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NPUの要件がINT8以上なので、コメントアウト\n",
    "convert_to_int4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_command: optimum-cli export openvino --model google/gemma-2b-it --task text-generation-with-past --weight-format int8 ../../model/gemma-2b-it/INT8\n",
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamashitaryuunosuke/TECH.C./コンテスト/OpenVINO2024/Taltner/server/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Framework not specified. Using pt to export the model.\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Error while downloading from https://cdn-lfs-us-1.huggingface.co/repos/07/bd/07bd0ee7b469afc0b9115a5c9fb06966403948914ff05454f8f9695190fa5cbc/561656f892a2a1ca0837ca529c5ce820a72b40f4f563b1cd0a1acc0b3899c30c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00001-of-00002.safetensors%3B+filename%3D%22model-00001-of-00002.safetensors%22%3B&Expires=1716704836&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjcwNDgzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzA3L2JkLzA3YmQwZWU3YjQ2OWFmYzBiOTExNWE1YzlmYjA2OTY2NDAzOTQ4OTE0ZmYwNTQ1NGY4Zjk2OTUxOTBmYTVjYmMvNTYxNjU2Zjg5MmEyYTFjYTA4MzdjYTUyOWM1Y2U4MjBhNzJiNDBmNGY1NjNiMWNkMGExYWNjMGIzODk5YzMwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Rz8f7Qwfzfz25JZd0We2Ze-xJPKgNi8cQ%7EJH1zxwmZOonjJyiHMzofw6VyQrdalD1LNUut9s-2qz1pCsG4STBaSEkQDyE9C9CHoEKiDiay7OuNEHYtNCUSb8K-dxIe0glLKmTx73LWM12ChfOOudNZN3ipbt8ryguR0Mx5XU5N2tkRBFuQyUUYon6-mUsmPGXLIBpQ1rhY2QyHg9fRmdG3lTsP2M5wwsQSbuzKcPuZ2VSitN1Y1CK7VEdZC2l4JUCp-6JkjuzNv-R2RAfQNUEq523i5EAH8L18B63%7ENrBe8mWSw2Qt-efWScZ72prfQEYVDbomd1VW-SKBGl3c897g__&Key-Pair-Id=KCD77M1F0VK2B: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs-us-1.huggingface.co/repos/07/bd/07bd0ee7b469afc0b9115a5c9fb06966403948914ff05454f8f9695190fa5cbc/561656f892a2a1ca0837ca529c5ce820a72b40f4f563b1cd0a1acc0b3899c30c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00001-of-00002.safetensors%3B+filename%3D%22model-00001-of-00002.safetensors%22%3B&Expires=1716704836&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjcwNDgzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzA3L2JkLzA3YmQwZWU3YjQ2OWFmYzBiOTExNWE1YzlmYjA2OTY2NDAzOTQ4OTE0ZmYwNTQ1NGY4Zjk2OTUxOTBmYTVjYmMvNTYxNjU2Zjg5MmEyYTFjYTA4MzdjYTUyOWM1Y2U4MjBhNzJiNDBmNGY1NjNiMWNkMGExYWNjMGIzODk5YzMwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Rz8f7Qwfzfz25JZd0We2Ze-xJPKgNi8cQ%7EJH1zxwmZOonjJyiHMzofw6VyQrdalD1LNUut9s-2qz1pCsG4STBaSEkQDyE9C9CHoEKiDiay7OuNEHYtNCUSb8K-dxIe0glLKmTx73LWM12ChfOOudNZN3ipbt8ryguR0Mx5XU5N2tkRBFuQyUUYon6-mUsmPGXLIBpQ1rhY2QyHg9fRmdG3lTsP2M5wwsQSbuzKcPuZ2VSitN1Y1CK7VEdZC2l4JUCp-6JkjuzNv-R2RAfQNUEq523i5EAH8L18B63%7ENrBe8mWSw2Qt-efWScZ72prfQEYVDbomd1VW-SKBGl3c897g__&Key-Pair-Id=KCD77M1F0VK2B: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs-us-1.huggingface.co/repos/07/bd/07bd0ee7b469afc0b9115a5c9fb06966403948914ff05454f8f9695190fa5cbc/561656f892a2a1ca0837ca529c5ce820a72b40f4f563b1cd0a1acc0b3899c30c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00001-of-00002.safetensors%3B+filename%3D%22model-00001-of-00002.safetensors%22%3B&Expires=1716704836&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjcwNDgzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzA3L2JkLzA3YmQwZWU3YjQ2OWFmYzBiOTExNWE1YzlmYjA2OTY2NDAzOTQ4OTE0ZmYwNTQ1NGY4Zjk2OTUxOTBmYTVjYmMvNTYxNjU2Zjg5MmEyYTFjYTA4MzdjYTUyOWM1Y2U4MjBhNzJiNDBmNGY1NjNiMWNkMGExYWNjMGIzODk5YzMwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Rz8f7Qwfzfz25JZd0We2Ze-xJPKgNi8cQ%7EJH1zxwmZOonjJyiHMzofw6VyQrdalD1LNUut9s-2qz1pCsG4STBaSEkQDyE9C9CHoEKiDiay7OuNEHYtNCUSb8K-dxIe0glLKmTx73LWM12ChfOOudNZN3ipbt8ryguR0Mx5XU5N2tkRBFuQyUUYon6-mUsmPGXLIBpQ1rhY2QyHg9fRmdG3lTsP2M5wwsQSbuzKcPuZ2VSitN1Y1CK7VEdZC2l4JUCp-6JkjuzNv-R2RAfQNUEq523i5EAH8L18B63%7ENrBe8mWSw2Qt-efWScZ72prfQEYVDbomd1VW-SKBGl3c897g__&Key-Pair-Id=KCD77M1F0VK2B: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    }
   ],
   "source": [
    "convert_to_int8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of FP16 model is 7288.13 MB\n",
      "Size of model with INT4 compressed weights is 2336.74 MB\n",
      "Compression rate for INT4 model: 3.119\n"
     ]
    }
   ],
   "source": [
    "# モデルが保存されているディレクトリのサイズを確認\n",
    "\n",
    "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "if fp16_weights.exists():\n",
    "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    if compressed_weights.exists() and fp16_weights.exists():\n",
    "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバイスの選択\n",
    "support_devices = core.available_devices\n",
    "device = 'NPU' if 'NPU' in support_devices else 'CPU'  # 実機のNPUが使えればいいのだけれど。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPU'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61a9b1377a6423994fc0675686a4057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to run:', options=('INT4', 'FP16'), value='INT4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "available_models = []\n",
    "if int4_model_dir.exists():\n",
    "    available_models.append(\"INT4\")\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "if fp16_model_dir.exists():\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INT4'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_run.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../../model/Phi-3-mini-4k-instruct/INT4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "\n",
    "if model_to_run.value == \"INT4\":  # 4bitモデルを使う場合\n",
    "    model_dir = int4_model_dir\n",
    "elif model_to_run.value == \"INT8\":  # 8bitモデルを使う場合\n",
    "    model_dir = int8_model_dir\n",
    "else:\n",
    "    model_dir = fp16_model_dir  # 16bitモデルを使う場合\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "ov_model = OVModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device=device,\n",
    "    ov_config=ov_config,\n",
    "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 指示\n",
      "あなたは、ユーザーが初対面の相手との会話を補助するためのアシスタントです。\n",
      "ユーザーと相手の会話を円滑に進めるために、ユーザーの補佐をします。\n",
      "以下の要素を考慮して、4つの話題をJSONフォーマットで提示しなさい。\n",
      "\n",
      "# 条件\n",
      "1. 初対面の相手との会話を始めるのに適した話題の提供\n",
      "2. 会話の流れを考慮して、次の話題を提示\n",
      "3. 単語くらいの短い文章で話題を提示\n",
      "4. 出力フォーマットは{{JSON}}形式\n",
      "\n",
      "以下のフォーマットで出力してください：\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"話題1\",\n",
      "    \"話題2\",\n",
      "    \"話題3\",\n",
      "    \"話題4\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# 例1\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"好きな食べ物\",\n",
      "    \"どこ出身\",\n",
      "    \"好きなゲーム\",\n",
      "    \"最近見た映画\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# 例2\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"趣味\",\n",
      "    \"休日の過ごし方\",\n",
      "    \"好きな音楽\",\n",
      "    \"行ってみたい場所\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# 例3\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"好きなスポーツ\",\n",
      "    \"好きなアニメ\",\n",
      "    \"好きな漫画\",\n",
      "    \"好きな映画\"\n",
      "  ]\n",
      "}\n",
      "``` 自分: 「初めまして〜。俺AI専攻の人と話すの初めてなんだけど”AI専攻ってどんなことしてるの”？」\n",
      "\n",
      "相手: 「AI専攻は機械学習とか画像や音声認識とかLLMについて勉強してるよ」\n",
      " ```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"AIにおける最近の進展\",\n",
      "    \"機械学習の基本的な概念\",\n",
      "\n",
      "    \"画像認識の応用例\",\n",
      "\n",
      "    \"音声認識の革新的な進展\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# モデルの動作確認\n",
    "tokenizer_kwargs = LLM_MODELS_CONFIG[model_name].get(\"tokenizer_kwargs\", {})\n",
    "\n",
    "SYSTEM_PROMPT = '''\\\n",
    "# 指示\n",
    "あなたは、ユーザーが初対面の相手との会話を補助するためのアシスタントです。\n",
    "ユーザーと相手の会話を円滑に進めるために、ユーザーの補佐をします。\n",
    "以下の要素を考慮して、4つの話題をJSONフォーマットで提示しなさい。\n",
    "\n",
    "# 条件\n",
    "1. 初対面の相手との会話を始めるのに適した話題の提供\n",
    "2. 会話の流れを考慮して、次の話題を提示\n",
    "3. 単語くらいの短い文章で話題を提示\n",
    "4. 出力フォーマットは{{JSON}}形式\n",
    "\n",
    "以下のフォーマットで出力してください：\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"話題1\",\n",
    "    \"話題2\",\n",
    "    \"話題3\",\n",
    "    \"話題4\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# 例1\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"好きな食べ物\",\n",
    "    \"どこ出身\",\n",
    "    \"好きなゲーム\",\n",
    "    \"最近見た映画\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# 例2\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"趣味\",\n",
    "    \"休日の過ごし方\",\n",
    "    \"好きな音楽\",\n",
    "    \"行ってみたい場所\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# 例3\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"好きなスポーツ\",\n",
    "    \"好きなアニメ\",\n",
    "    \"好きな漫画\",\n",
    "    \"好きな映画\"\n",
    "  ]\n",
    "}\n",
    "```\\\n",
    "'''\n",
    "\n",
    "conversation = '''\\\n",
    "自分: 「初めまして〜。俺AI専攻の人と話すの初めてなんだけど”AI専攻ってどんなことしてるの”？」\n",
    "\n",
    "相手: 「AI専攻は機械学習とか画像や音声認識とかLLMについて勉強してるよ」\n",
    "'''\n",
    "\n",
    "start_message: str = LLM_MODELS_CONFIG[model_name]['start_message']\n",
    "prompt_template: str = LLM_MODELS_CONFIG[model_name]['prompt_template']\n",
    "\n",
    "sys_prompt = start_message.format(\n",
    "    SYSTEM_PROMPT=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "prompt = sys_prompt + prompt_template.format(\n",
    "    user=conversation\n",
    ")\n",
    "\n",
    "input_tokens = tok(prompt, return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "\n",
    "answer = ov_model.generate(\n",
    "    **input_tokens,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "ans = tok.batch_decode(answer, skip_special_tokens=True)[0]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
