{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依存関係\n",
    "'''\n",
    "poetry add wheel\n",
    "\n",
    "poetry add --group dev 'ipykernel' 'ipywidgets'\n",
    "\n",
    "poetry add --group llm 'openvino'\\\n",
    "    \"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "    \"git+https://github.com/openvinotoolkit/nncf.git\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from llm_config import LLM_MODELS_CONFIG\n",
    "\n",
    "import openvino as ov\n",
    "import nncf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modelがgemmaの場合は、ログインが必要\n",
    "def login_huggingface_hub():\n",
    "    from huggingface_hub import notebook_login, whoami\n",
    "    try:\n",
    "        whoami()\n",
    "        print('Authorization token already provided')\n",
    "    except OSError:\n",
    "        notebook_login()\n",
    "        print('Authorization token has been provided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nncf.set_log_level(logging.ERROR)\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "# デバイスの特定\n",
    "support_devices = core.available_devices\n",
    "device = 'NPU' if 'NPU' in support_devices else 'CPU'\n",
    "\n",
    "\n",
    "# ダウンロードするモデル\n",
    "# NPUならPhi-3-mini-4k-instruct, CPUならgemma-2b-it\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\" if device == 'NPU' else \"google/gemma-2b-it\"\n",
    "\n",
    "model_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "## modelがgemmaの場合は、HuggingFaceのログインが必要\n",
    "if model_id == \"google/gemma-2b-it\":\n",
    "    login_huggingface_hub()\n",
    "\n",
    "model_name = model_id.split('/')[-1]\n",
    "\n",
    "remote_code = LLM_MODELS_CONFIG[model_name]['remote_code']  # Phi-3はTrue\n",
    "\n",
    "# モデルを保存するディレクトリ\n",
    "model_dir = Path(f'../../model/{model_name}')\n",
    "fp16_model_dir = model_dir / \"FP16\"  # float 16bitモデルの保存先\n",
    "int8_model_dir = model_dir / \"INT8\"  # 量子化モデルの保存先(8bit)\n",
    "# # NPUの要件がINT8以上なので、コメントアウト\n",
    "# int4_model_dir = model_dir / \"INT4\"  # 量子化モデルの保存先(4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NPUの要件がINT8以上なので、コメントアウト\n",
    "\n",
    "# # INT4の設定\n",
    "# compression_configs = {\n",
    "#     # ここのパラメータは要調整\n",
    "#     # \"sym\":          対称量子化の利用\n",
    "#     # 'group_size':  グループサイズ  (64, 128が無難？)\n",
    "#     # 'ratio':       量子化後のパラメータの割合  (0.5~0.8で試す)\n",
    "#     \"gemma-2b-it\": {\n",
    "#         \"sym\": True,\n",
    "#         \"group_size\": 64,\n",
    "#         \"ratio\": 0.6,\n",
    "#     },\n",
    "#     \"default\": {\n",
    "#         \"sym\": False,\n",
    "#         \"group_size\": 128,\n",
    "#         \"ratio\": 0.8,\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimum-cliでモデルをopenvino形式でダウンロード\n",
    "export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past\".format(model_id)\n",
    "\n",
    "def convert_to_fp16():\n",
    "    global export_command_base\n",
    "    export_command = ''\n",
    "    # すでに存在する場合はスキップ\n",
    "    if (fp16_model_dir / \"openvino_model.xml\" ).exists():\n",
    "        return\n",
    "    if remote_code:\n",
    "        # Phi-3のみ\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" --weight-format fp16\"\n",
    "    export_command += \" \" + str(fp16_model_dir)\n",
    "    # モデルのダウンロード開始時間\n",
    "    start_model_download = datetime.now()\n",
    "    print('export_command:', export_command)\n",
    "    os.system(export_command)  # モデルのダウンロード\n",
    "    # モデルのダウンロード終了時間\n",
    "    end_model_download = datetime.now() - start_model_download\n",
    "    print('export done', end_model_download.total_seconds())\n",
    "\n",
    "\n",
    "def convert_to_int8():\n",
    "    global export_command_base\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" --weight-format int8\"\n",
    "    export_command += \" \" + str(int8_model_dir)\n",
    "    # モデルのダウンロード開始時間\n",
    "    start_model_download = datetime.now()\n",
    "    print('export_command:', export_command)\n",
    "    os.system(export_command)  # モデルのダウンロード\n",
    "    # モデルのダウンロード終了時間\n",
    "    end_model_download = datetime.now() - start_model_download\n",
    "    print('export done', end_model_download.total_seconds())\n",
    "\n",
    "\n",
    "# # NPUの要件がINT8以上なので、コメントアウト\n",
    "# def convert_to_int4():\n",
    "#     global export_command_base\n",
    "#     if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "#         return\n",
    "#     if remote_code:\n",
    "#         export_command_base += \" --trust-remote-code\"\n",
    "#     # 量子化の設定\n",
    "#     model_compression_params  = compression_configs.get(model_name, compression_configs[\"default\"])\n",
    "#     export_command = export_command_base + \" --weight-format int4\"\n",
    "#     int4_compression_args = \" --group-size {} --ratio {}\".format(model_compression_params[\"group_size\"], model_compression_params[\"ratio\"])\n",
    "#     if model_compression_params[\"sym\"]:\n",
    "#         int4_compression_args += \" --sym\"\n",
    "#     export_command += int4_compression_args + \" \" + str(int4_model_dir)\n",
    "#     # モデルのダウンロード開始時間\n",
    "#     start_model_download = datetime.now()\n",
    "#     print('export_command:', export_command)\n",
    "#     os.system(export_command)  # モデルのダウンロード\n",
    "#     # モデルのダウンロード終了時間\n",
    "#     end_model_download = datetime.now() - start_model_download\n",
    "#     print('export done', end_model_download.total_seconds())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NPUの要件がINT8以上なので、コメントアウト\n",
    "# convert_to_int4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_int8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルが保存されているディレクトリのサイズを確認\n",
    "\n",
    "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "# int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "if fp16_weights.exists():\n",
    "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# ファイルのサイズを確認\n",
    "# for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "for precision, compressed_weights in zip([8], [int8_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    if compressed_weights.exists() and fp16_weights.exists():\n",
    "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバイスの選択\n",
    "support_devices = core.available_devices\n",
    "device = 'NPU' if 'NPU' in support_devices else 'CPU'  # 実機のNPUが使えればいいのだけれど。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "available_models = []\n",
    "# if int4_model_dir.exists():\n",
    "#     available_models.append(\"INT4\")\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "if fp16_model_dir.exists():\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_run.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "\n",
    "# if model_to_run.value == \"INT4\":  # 4bitモデルを使う場合\n",
    "#     model_dir = int4_model_dir\n",
    "if model_to_run.value == \"INT8\":  # 8bitモデルを使う場合\n",
    "    model_dir = int8_model_dir\n",
    "else:\n",
    "    model_dir = fp16_model_dir  # 16bitモデルを使う場合\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "ov_model = OVModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device=device,\n",
    "    ov_config=ov_config,\n",
    "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt\n",
    "SYSTEM_PROMPT = '''\\\n",
    "# 指示\n",
    "あなたは、ユーザーが初対面の相手との会話を補助するためのアシスタントです。\n",
    "ユーザーと相手の会話を円滑に進めるために、ユーザーの補佐をします。\n",
    "以下の要素を考慮して、**4つの話題をJSONフォーマット**で提示しなさい。\n",
    "\n",
    "# 条件\n",
    "1. 初対面の相手との会話を始めるのに適した話題の提供\n",
    "2. 会話の流れを考慮して、次の話題を提示\n",
    "3. 単語レベルの短い文章で話題を提示\n",
    "4. 4つの話題を提示\n",
    "5. 出力フォーマットは{{JSON}}形式\n",
    "6. 出力は単一のJSON\n",
    "\n",
    "**以下のフォーマットで出力しなさい**\n",
    "\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"話題1\",\n",
    "    \"話題2\",\n",
    "    \"話題3\",\n",
    "    \"話題4\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "# 例1\n",
    "## 入力\n",
    "\n",
    "自分「○○さん、こんにちは。初めまして。私はAといいます。」\n",
    "相手「こんにちは。私はBです。よろしくお願いします!」\n",
    "\n",
    "## 出力\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"年齢\",\n",
    "    \"どこ出身\",\n",
    "    \"趣味\",\n",
    "    \"好きな食べ物\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "# 例2\n",
    "## 入力\n",
    "自分「○○さんって、どんなことが好きですか？」\n",
    "相手「音楽が好きで、よくライブに行ってます。」\n",
    "\n",
    "## 出力\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"好きなアーティスト\",\n",
    "    \"休日の過ごし方\",\n",
    "    \"好きな音楽のジャンル\",\n",
    "    \"最近行ったライブ\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "# 例3\n",
    "## 入力\n",
    "自分「○○さん、最近見た映画はありますか？」\n",
    "相手「最近○○って映画を見たんですけど、面白かったですよ。」\n",
    "\n",
    "## 出力\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"映画のジャンル\",\n",
    "    \"主演俳優\",\n",
    "    \"好きな映画のジャンル\",\n",
    "    \"一番好きな映画\"\n",
    "  ]\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの動作確認\n",
    "tokenizer_kwargs = LLM_MODELS_CONFIG[model_name].get(\"tokenizer_kwargs\", {})\n",
    "\n",
    "\n",
    "\n",
    "conversation = '''\\\n",
    "自分: 「初めまして〜。俺AI専攻の人と話すの初めてなんだけどAI専攻ってどんなことしてるの？」\n",
    "\n",
    "相手: 「AI専攻は機械学習とか画像や音声認識とかLLMについて勉強してるよ」\n",
    "'''\n",
    "\n",
    "start_message: str = LLM_MODELS_CONFIG[model_name]['start_message']\n",
    "prompt_template: str = LLM_MODELS_CONFIG[model_name]['prompt_template']\n",
    "\n",
    "sys_prompt = start_message.format(\n",
    "    SYSTEM_PROMPT=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "prompt = sys_prompt + prompt_template.format(\n",
    "    user=conversation\n",
    ")\n",
    "\n",
    "input_tokens = tok(prompt, return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "\n",
    "answer = ov_model.generate(\n",
    "    **input_tokens,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "ans = tok.batch_decode(answer, skip_special_tokens=True)[0]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
