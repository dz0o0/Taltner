{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNPU:\\n    Phi-3-mini-4k-instruct\\n        INT8?FP16?\\nCPU:\\n    gemma-2b-it\\n        INT4\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 依存関係\n",
    "'''\n",
    "poetry add wheel\n",
    "\n",
    "poetry add --group dev 'ipykernel' 'ipywidgets'\n",
    "\n",
    "poetry add --group llm 'openvino'\\\n",
    "    \"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "    \"git+https://github.com/openvinotoolkit/nncf.git\"\n",
    "'''\n",
    "\n",
    "# 使用モデル\n",
    "'''\n",
    "NPU:\n",
    "    Phi-3-mini-4k-instruct\n",
    "        INT8?FP16?\n",
    "CPU:\n",
    "    gemma-2b-it\n",
    "        INT4\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from llm_config import LLM_MODELS_CONFIG\n",
    "\n",
    "import openvino as ov\n",
    "import nncf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modelがgemmaの場合は、ログインが必要\n",
    "def login_huggingface_hub():\n",
    "    from huggingface_hub import notebook_login, whoami\n",
    "    try:\n",
    "        whoami()\n",
    "        print('Authorization token already provided')\n",
    "    except OSError:\n",
    "        notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authorization token already provided\n"
     ]
    }
   ],
   "source": [
    "nncf.set_log_level(logging.ERROR)\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "# デバイスの特定\n",
    "support_devices = core.available_devices\n",
    "device = 'NPU' if 'NPU' in support_devices else 'CPU'  # 実機のNPUが使えればいいのだけれど。。。\n",
    "\n",
    "# ダウンロードするモデル\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\" if device == 'NPU' else \"google/gemma-2b-it\"\n",
    "\n",
    "# 一旦テストでPhi-3-mini-4k-instructを使う\n",
    "# model_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "## modelがgemmaの場合は、HuggingFaceのログインが必要\n",
    "if model_id == \"google/gemma-2b-it\":\n",
    "    login_huggingface_hub()\n",
    "\n",
    "model_name = model_id.split('/')[-1]\n",
    "\n",
    "remote_code = LLM_MODELS_CONFIG[model_name]['remote_code']  # Phi-3はTrue\n",
    "\n",
    "# モデルを保存するディレクトリ\n",
    "model_dir = Path(f'../../model/{model_name}')\n",
    "fp16_model_dir = model_dir / \"FP16\"  # float 16bitモデルの保存先\n",
    "int8_model_dir = model_dir / \"INT8\"  # 量子化モデルの保存先(8bit)\n",
    "int4_model_dir = model_dir / \"INT4\"  # 量子化モデルの保存先(4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT4の設定\n",
    "compression_configs = {\n",
    "    # ここのパラメータは要調整\n",
    "    # \"sym\":          対称量子化の利用\n",
    "    # 'group_size':  グループサイズ  (64, 128が無難？)\n",
    "    # 'ratio':       量子化後のパラメータの割合  (0.5~0.8で試す)\n",
    "    \"gemma-2b-it\": {\n",
    "        \"sym\": True,\n",
    "        \"group_size\": 64,\n",
    "        \"ratio\": 0.6,\n",
    "    },\n",
    "    \"default\": {\n",
    "        \"sym\": False,\n",
    "        \"group_size\": 128,\n",
    "        \"ratio\": 0.8,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google/gemma-2b-it'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimum-cliでモデルをopenvino形式でダウンロード\n",
    "export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past\".format(model_id)\n",
    "\n",
    "def convert_to_fp16():\n",
    "    global export_command_base\n",
    "    export_command = ''\n",
    "    # すでに存在する場合はスキップ\n",
    "    if (fp16_model_dir / \"openvino_model.xml\" ).exists():\n",
    "        return\n",
    "    if remote_code:\n",
    "        # Phi-3のみ\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" --weight-format fp16\"\n",
    "    export_command += \" \" + str(fp16_model_dir)\n",
    "    # モデルのダウンロード開始時間\n",
    "    start_model_download = datetime.now()\n",
    "    print('export_command:', export_command)\n",
    "    os.system(export_command)  # モデルのダウンロード\n",
    "    # モデルのダウンロード終了時間\n",
    "    end_model_download = datetime.now() - start_model_download\n",
    "    print('export done', end_model_download.total_seconds())\n",
    "\n",
    "\n",
    "def convert_to_int8():\n",
    "    global export_command_base\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" --weight-format int8\"\n",
    "    export_command += \" \" + str(int8_model_dir)\n",
    "    # モデルのダウンロード開始時間\n",
    "    start_model_download = datetime.now()\n",
    "    print('export_command:', export_command)\n",
    "    os.system(export_command)  # モデルのダウンロード\n",
    "    # モデルのダウンロード終了時間\n",
    "    end_model_download = datetime.now() - start_model_download\n",
    "    print('export done', end_model_download.total_seconds())\n",
    "\n",
    "def convert_to_int4():\n",
    "    global export_command_base\n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    # 量子化の設定\n",
    "    model_compression_params  = compression_configs.get(model_name, compression_configs[\"default\"])\n",
    "    export_command = export_command_base + \" --weight-format int4\"\n",
    "    int4_compression_args = \" --group-size {} --ratio {}\".format(model_compression_params[\"group_size\"], model_compression_params[\"ratio\"])\n",
    "    if model_compression_params[\"sym\"]:\n",
    "        int4_compression_args += \" --sym\"\n",
    "    export_command += int4_compression_args + \" \" + str(int4_model_dir)\n",
    "    # モデルのダウンロード開始時間\n",
    "    start_model_download = datetime.now()\n",
    "    print('export_command:', export_command)\n",
    "    os.system(export_command)  # モデルのダウンロード\n",
    "    # モデルのダウンロード終了時間\n",
    "    end_model_download = datetime.now() - start_model_download\n",
    "    print('export done', end_model_download.total_seconds())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_command: optimum-cli export openvino --model google/gemma-2b-it --task text-generation-with-past --weight-format int4 --group-size 64 --ratio 0.6 --sym ../../model/gemma-2b-it/INT4\n",
      "export done 0.026528\n"
     ]
    }
   ],
   "source": [
    "# deviceがNPUならINT8, それ以外はINT4\n",
    "convert_to_int8() if device == \"NPU\" else convert_to_int4()\n",
    "# convert_to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルが保存されているディレクトリのサイズを確認\n",
    "\n",
    "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "# int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "if fp16_weights.exists():\n",
    "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# ファイルのサイズを確認\n",
    "# for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "for precision, compressed_weights in zip([8], [int8_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    if compressed_weights.exists() and fp16_weights.exists():\n",
    "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPU'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070d44bc933c478daa9136c6295d8500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to run:', options=('INT8', 'FP16'), value='INT8')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "available_models = []\n",
    "# if int4_model_dir.exists():\n",
    "#     available_models.append(\"INT4\")\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "if fp16_model_dir.exists():\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INT8'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_run.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../../model/Phi-3-mini-4k-instruct/INT8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "\n",
    "# if model_to_run.value == \"INT4\":  # 4bitモデルを使う場合\n",
    "#     model_dir = int4_model_dir\n",
    "if model_to_run.value == \"INT8\":  # 8bitモデルを使う場合\n",
    "    model_dir = int8_model_dir\n",
    "else:\n",
    "    model_dir = fp16_model_dir  # 16bitモデルを使う場合\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "ov_model = OVModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device=device,\n",
    "    ov_config=ov_config,\n",
    "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt\n",
    "SYSTEM_PROMPT = '''\\\n",
    "# Instructions\n",
    "You are an assistant designed to help users facilitate conversations with people they are meeting for the first time.\n",
    "You will assist the user by smoothing the conversation between them and the other party.\n",
    "Consider the following elements and present **four topics in JSON format**.\n",
    "\n",
    "# Conditions\n",
    "1. Provide topics suitable for conversation with a person you are meeting for the first time.\n",
    "2. Consider the flow of the conversation when presenting the next topic.\n",
    "3. Present topics in {{short content at the word level}}.\n",
    "4. Output language should be {{Japanese}}.\n",
    "\n",
    "**Present in the following format**\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"Topic 1\",\n",
    "    \"Topic 2\",\n",
    "    \"Topic 3\",\n",
    "    \"Topic 4\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 1\n",
    "## Input\n",
    "自分「最近ハイキングにハマってるんだけど、君もアウトドア活動は好き？」\n",
    "相手「うん、たまには山に登ったりするよ。」\n",
    "\n",
    "## Output\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"おすすめのハイキングスポット\",\n",
    "    \"必要な装備\",\n",
    "    \"最近の登山経験\",\n",
    "    \"季節ごとの活動\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 2\n",
    "## Input\n",
    "自分「サスペンス映画が好きでよく見るんだけど、好きなジャンルは？」\n",
    "相手「僕はドキュメンタリーが好きだね。」\n",
    "\n",
    "## Output\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"おすすめのドキュメンタリー\",\n",
    "    \"サスペンス映画の魅力\",\n",
    "    \"映画鑑賞の場所\",\n",
    "    \"映画の感想交換\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 3\n",
    "## Input\n",
    "自分「テクノロジーのニュースを追ってるんだけど、興味はある？」\n",
    "相手「うん、特にAIに関しては詳しく知りたい。」\n",
    "\n",
    "## Output\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"最新のAI技術\",\n",
    "    \"AIの社会的影響\",\n",
    "    \"読んでるニュースのサイト\",\n",
    "    \"AIに関するおすすめの本\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 4\n",
    "## Input\n",
    "自分「コーヒーを淹れるのが趣味なんだけど、君はコーヒーは好きかな？」\n",
    "相手「大好きだよ。いつもカフェで新しい豆を試してる。」\n",
    "\n",
    "## Output\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"おすすめのコーヒー豆\",\n",
    "    \"カフェのおすすめ\",\n",
    "    \"自宅でのコーヒーの淹れ方\",\n",
    "    \"コーヒーと健康\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 5\n",
    "## Input\n",
    "自分「SNSで美術館の展示を見るのが好きなんだけど、美術館は好き？」\n",
    "相手「美術館はいいよね、特に近代美術が好き。」\n",
    "\n",
    "## Output\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"おすすめの近代美術館\",\n",
    "    \"最近訪れた展示\",\n",
    "    \"アートコレクション\",\n",
    "    \"美術館のおすすめ展示\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 6\n",
    "## Input\n",
    "自分「自分でミュージックを作るのが趣味なんだけど、君も音楽は好き？」\n",
    "相手「音楽は大好きで、よくライブに行くよ。」\n",
    "\n",
    "## Output\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"おすすめのアーティスト\",\n",
    "    \"最近のライブ体験\",\n",
    "    \"音楽制作のヒント\",\n",
    "    \"音楽ジャンルの好み\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 7\n",
    "## Input\n",
    "自分「料理のYouTubeチャンネルを見るのが好きなんだけど、君はどんなチャンネルを見る？」\n",
    "相手「テクノロジーレビューのチャンネルをよく見てるよ。」\n",
    "\n",
    "## Output\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"おすすめのテクノロジーチャンネル\",\n",
    "    \"テクノロジーチャンネルを見る理由\",\n",
    "    \"YouTubeで学べること\",\n",
    "    \"最新のテクノロジートレンド\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 8\n",
    "## Input\n",
    "自分「海外旅行が趣味で、最近はアジアを中心に回ってるんだ。君も旅行は好き？」\n",
    "相手「うん、特にヨーロッパが好きで、よく行くよ。」\n",
    "\n",
    "## Output\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"おすすめのヨーロッパの国\",\n",
    "    \"ヨーロッパ旅行の魅力\",\n",
    "    \"旅行の計画方法\",\n",
    "    \"旅行でのエピソード\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 9\n",
    "## Input\n",
    "自分「仕事でプログラミングをしてるんだけど、君もIT関連の仕事をしてる？」\n",
    "相手「はい、システムエンジニアとして働いています。」\n",
    "\n",
    "## Output\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"プログラミング言語の選択\",\n",
    "    \"プロジェクトの課題\",\n",
    "    \"IT業界のトレンド\",\n",
    "    \"キャリアパスの話\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 10\n",
    "## Input\n",
    "自分「ブログを書いていて、主に旅行についての情報を発信してるんだ。君も何か書いてる？」\n",
    "相手「写真についてのブログをたまに更新してるよ。」\n",
    "\n",
    "## Output\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"ブログのコンテンツアイデア\",\n",
    "    \"写真技術のポイント\",\n",
    "    \"旅行の記録\",\n",
    "    \"SNSでの情報共有\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの動作確認\n",
    "tokenizer_kwargs = LLM_MODELS_CONFIG[model_name].get(\"tokenizer_kwargs\", {})\n",
    "\n",
    "conversation = '''\\\n",
    "自分「機械学習の授業俺も興味あるんだけど難易度どんな感じ？」\n",
    "相手「結構難しいと思うけど、興味あるなら楽しめると思うよ」\n",
    "'''\n",
    "\n",
    "start_message: str = LLM_MODELS_CONFIG[model_name]['start_message']\n",
    "prompt_template: str = LLM_MODELS_CONFIG[model_name]['prompt_template']\n",
    "\n",
    "sys_prompt = start_message.format(\n",
    "    SYSTEM_PROMPT=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "prompt = sys_prompt + prompt_template.format(\n",
    "    user=conversation\n",
    ")\n",
    "\n",
    "input_tokens = tok(prompt, return_tensors=\"pt\", **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instructions\n",
      "You are an assistant designed to help users facilitate conversations with people they are meeting for the first time.\n",
      "You will assist the user by smoothing the conversation between them and the other party.\n",
      "Consider the following elements and present **four topics in JSON format**.\n",
      "\n",
      "# Conditions\n",
      "1. Provide topics suitable for conversation with a person you are meeting for the first time.\n",
      "2. Consider the flow of the conversation when presenting the next topic.\n",
      "3. Present topics in {{short content at the word level}}.\n",
      "4. Output language should be {{Japanese}}.\n",
      "\n",
      "**Present in the following format**\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"Topic 1\",\n",
      "    \"Topic 2\",\n",
      "    \"Topic 3\",\n",
      "    \"Topic 4\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# Example 1\n",
      "## Input\n",
      "自分「最近ハイキングにハマってるんだけど、君もアウトドア活動は好き？」\n",
      "相手「うん、たまには山に登ったりするよ。」\n",
      "\n",
      "## Output\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"おすすめのハイキングスポット\",\n",
      "    \"必要な装備\",\n",
      "    \"最近の登山経験\",\n",
      "    \"季節ごとの活動\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# Example 2\n",
      "## Input\n",
      "自分「サスペンス映画が好きでよく見るんだけど、好きなジャンルは？」\n",
      "相手「僕はドキュメンタリーが好きだね。」\n",
      "\n",
      "## Output\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"おすすめのドキュメンタリー\",\n",
      "    \"サスペンス映画の魅力\",\n",
      "    \"映画鑑賞の場所\",\n",
      "    \"映画の感想交換\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# Example 3\n",
      "## Input\n",
      "自分「テクノロジーのニュースを追ってるんだけど、興味はある？」\n",
      "相手「うん、特にAIに関しては詳しく知りたい。」\n",
      "\n",
      "## Output\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"最新のAI技術\",\n",
      "    \"AIの社会的影響\",\n",
      "    \"読んでるニュースのサイト\",\n",
      "    \"AIに関するおすすめの本\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# Example 4\n",
      "## Input\n",
      "自分「コーヒーを淹れるのが趣味なんだけど、君はコーヒーは好きかな？」\n",
      "相手「大好きだよ。いつもカフェで新しい豆を試してる。」\n",
      "\n",
      "## Output\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"おすすめのコーヒー豆\",\n",
      "    \"カフェのおすすめ\",\n",
      "    \"自宅でのコーヒーの淹れ方\",\n",
      "    \"コーヒーと健康\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# Example 5\n",
      "## Input\n",
      "自分「SNSで美術館の展示を見るのが好きなんだけど、美術館は好き？」\n",
      "相手「美術館はいいよね、特に近代美術が好き。」\n",
      "\n",
      "## Output\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"おすすめの近代美術館\",\n",
      "    \"最近訪れた展示\",\n",
      "    \"アートコレクション\",\n",
      "    \"美術館のおすすめ展示\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# Example 6\n",
      "## Input\n",
      "自分「自分でミュージックを作るのが趣味なんだけど、君も音楽は好き？」\n",
      "相手「音楽は大好きで、よくライブに行くよ。」\n",
      "\n",
      "## Output\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"おすすめのアーティスト\",\n",
      "    \"最近のライブ体験\",\n",
      "    \"音楽制作のヒント\",\n",
      "    \"音楽ジャンルの好み\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# Example 7\n",
      "## Input\n",
      "自分「料理のYouTubeチャンネルを見るのが好きなんだけど、君はどんなチャンネルを見る？」\n",
      "相手「テクノロジーレビューのチャンネルをよく見てるよ。」\n",
      "\n",
      "## Output\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"おすすめのテクノロジーチャンネル\",\n",
      "    \"テクノロジーチャンネルを見る理由\",\n",
      "    \"YouTubeで学べること\",\n",
      "    \"最新のテクノロジートレンド\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# Example 8\n",
      "## Input\n",
      "自分「海外旅行が趣味で、最近はアジアを中心に回ってるんだ。君も旅行は好き？」\n",
      "相手「うん、特にヨーロッパが好きで、よく行くよ。」\n",
      "\n",
      "## Output\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"おすすめのヨーロッパの国\",\n",
      "    \"ヨーロッパ旅行の魅力\",\n",
      "    \"旅行の計画方法\",\n",
      "    \"旅行でのエピソード\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# Example 9\n",
      "## Input\n",
      "自分「仕事でプログラミングをしてるんだけど、君もIT関連の仕事をしてる？」\n",
      "相手「はい、システムエンジニアとして働いています。」\n",
      "\n",
      "## Output\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"プログラミング言語の選択\",\n",
      "    \"プロジェクトの課題\",\n",
      "    \"IT業界のトレンド\",\n",
      "    \"キャリアパスの話\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# Example 10\n",
      "## Input\n",
      "自分「ブログを書いていて、主に旅行についての情報を発信してるんだ。君も何か書いてる？」\n",
      "相手「写真についてのブログをたまに更新してるよ。」\n",
      "\n",
      "## Output\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"ブログのコンテンツアイデア\",\n",
      "    \"写真技術のポイント\",\n",
      "    \"旅行の記録\",\n",
      "    \"SNSでの情報共有\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      " 自分「機械学習の授業俺も興味あるんだけど難易度どんな感じ？」\n",
      "相手「結構難しいと思うけど、興味あるなら楽しめると思うよ」 ```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"基本的な機械学習技術\",\n",
      "    \"困難な問題解決の戦略\",\n",
      "    \"実用的な機械学習の事例\",\n",
      "    \"授業の質の向上方法\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "CPU times: user 6min 1s, sys: 22.8 s, total: 6min 24s\n",
      "Wall time: 42.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "answer = ov_model.generate(\n",
    "    **input_tokens,\n",
    "    max_new_tokens=110,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "decoded_answer = tok.batch_decode(answer, skip_special_tokens=True)[0]\n",
    "print(decoded_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aiが生成したテキスト部分のみを取得\n",
    "def extract_generated_text(decoded_answer: str) -> str:\n",
    "    generated_text = []\n",
    "    capturing = False\n",
    "    append_closure = False\n",
    "    closure_text = \"\"\n",
    "\n",
    "    lines = decoded_answer.strip().split('\\n')[::-1]\n",
    "\n",
    "    # コードブロックが正常に終了しない場合に使用\n",
    "    def extract_json_text(lines):\n",
    "        generated_text = []\n",
    "        for line in lines:\n",
    "            if \"```json\" in line:\n",
    "                return generated_text\n",
    "            generated_text.append(line)\n",
    "\n",
    "\n",
    "    # テキストを行ごとに分割して逆順で処理\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        # JSONでコードブロックの終わりまで正常に出力される場合　　\n",
    "        # コードブロックの終わりを見つけた場合\n",
    "        if line.strip() == \"```\":\n",
    "            capturing = True\n",
    "            continue  # 次の行から取得開始\n",
    "\n",
    "        # コードブロックの始まりを見つけたら終了\n",
    "        if capturing and \"```json\" in stripped_line:\n",
    "            print('正常に終了')\n",
    "            break\n",
    "\n",
    "        if capturing:\n",
    "            generated_text.append(line)\n",
    "\n",
    "        #　コードブロックの終わりまで正常に出力されない場合、特定の状態で処理\n",
    "        if (not capturing) and (\"```json\" in stripped_line):\n",
    "            end_line = lines[0]\n",
    "            # 最後の行が'}'なら、そこと、次の'```json'の間を取得\n",
    "            if '}' in end_line:\n",
    "                print('最後の行が\"}\"')\n",
    "                append_closure = False\n",
    "                generated_text = extract_json_text(lines)\n",
    "            # 最後の行が']'なら、そこと、次の'```json'の間を取得し、最後に'\\n}'を追加\n",
    "            elif \"]\" in end_line:\n",
    "                print('最後の行が\"]\"')\n",
    "                append_closure = True\n",
    "                closure_text = \"\\n}\"\n",
    "                generated_text = extract_json_text(lines)\n",
    "            # それ以外は、次の'```json'の間を取得し、最後が'\"'なら'\\n]\\n}'を追加、それ以外は'\"\\n]\\n}'を追加\n",
    "            else:\n",
    "                print('最後の行がそれ以外')\n",
    "                generated_text = extract_json_text(lines)\n",
    "                if end_line.strip().endswith(\"\\\"\"):\n",
    "                    append_closure = True\n",
    "                    closure_text = '\\n  ]\\n}'\n",
    "                else:\n",
    "                    append_closure = True\n",
    "                    closure_text = '\\\"\\n  ]\\n}'\n",
    "            break\n",
    "\n",
    "    reversed_text = \"\\n\".join(reversed(generated_text))\n",
    "\n",
    "    if append_closure:\n",
    "        reversed_text += closure_text\n",
    "\n",
    "    return reversed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最後の行がそれ以外\n",
      "\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "decoded_answer_example = '''\n",
    "## Output\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"ブログのコンテンツアイデア\",\n",
    "    \"写真技術のポイント\",\n",
    "    \"旅行の記録\",\n",
    "    \"SNSでの情報共有\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    " 自分「機械学習の授業俺も興味あるんだけど難易度どんな感じ？」\n",
    "相手「結構難しいと思うけど、興味あるなら楽しめると思うよ」 ```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"基本的な機械学習技術\",\n",
    "    \"困難な問題解決の戦略\",\n",
    "    \"実用的な機械学習の事例\",\n",
    "    \"授業の質の向上方法\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "'''\n",
    "\n",
    "# 最後が```jsonで終わってしまう場合、正常に終了しないから、そこを明日直す\n",
    "\n",
    "# 実際の関数を使用して結果を得る\n",
    "result = extract_generated_text(decoded_answer_example)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
