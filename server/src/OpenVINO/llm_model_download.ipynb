{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npoetry add wheel\\n\\npoetry add --group dev \\'ipykernel\\' \\'ipywidgets\\'\\n\\npoetry add --group llm \\'openvino\\'    \"git+https://github.com/huggingface/optimum-intel.git\"    \"git+https://github.com/openvinotoolkit/nncf.git\"\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 依存関係\n",
    "'''\n",
    "poetry add wheel\n",
    "\n",
    "poetry add --group dev 'ipykernel' 'ipywidgets'\n",
    "\n",
    "poetry add --group llm 'openvino'\\\n",
    "    \"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "    \"git+https://github.com/openvinotoolkit/nncf.git\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from llm_config import LLM_MODELS_CONFIG\n",
    "\n",
    "import openvino as ov\n",
    "import nncf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## login to huggingfacehub to get access to pretrained model \n",
    "\n",
    "# from huggingface_hub import notebook_login, whoami\n",
    "\n",
    "# try:\n",
    "#     whoami()\n",
    "#     print('Authorization token already provided')\n",
    "# except OSError:\n",
    "#     notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nncf.set_log_level(logging.ERROR)\n",
    "\n",
    "# ダウンロードするモデル\n",
    "company = \"microsoft\"\n",
    "# company = \"google\"\n",
    "# company = \"rinna\"\n",
    "\n",
    "model_name = \"Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"gemma-2b-it\"\n",
    "# model_name = \"youri-7b-chat\"\n",
    "\n",
    "model_id = f'{company}/{model_name}'\n",
    "\n",
    "remote_code = LLM_MODELS_CONFIG[model_name]['remote_code']  # Phi-3はTrue\n",
    "\n",
    "# モデルを保存するディレクトリ\n",
    "model_dir = Path(f'../../model/{model_name}')\n",
    "fp16_model_dir = model_dir / \"FP16\"  # float 16bitモデルの保存先\n",
    "int8_model_dir = model_dir / \"INT8\"  # 量子化モデルの保存先(8bit)\n",
    "int4_model_dir = model_dir / \"INT4\"  # 量子化モデルの保存先(4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_configs = {\n",
    "    # ここのパラメータは要調整\n",
    "    # \"sym\":          対称量子化の利用\n",
    "    # 'group_size':  グループサイズ  (64, 128が無難？)\n",
    "    # 'ratio':       量子化後のパラメータの割合  (0.5~0.8で試す)\n",
    "    \"gemma-2b-it\": {\n",
    "        \"sym\": True,\n",
    "        \"group_size\": 64,\n",
    "        \"ratio\": 0.6,\n",
    "    },\n",
    "    \"default\": {\n",
    "        \"sym\": False,\n",
    "        \"group_size\": 128,\n",
    "        \"ratio\": 0.8,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft/Phi-3-mini-4k-instruct'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../../model/Phi-3-mini-4k-instruct/FP16')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp16_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "# optimum-cliでモデルをopenvino形式でダウンロード\n",
    "export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past\".format(model_id)\n",
    "\n",
    "def convert_to_fp16():\n",
    "    global export_command_base\n",
    "    export_command = ''\n",
    "    # すでに存在する場合はスキップ\n",
    "    if (fp16_model_dir / \"openvino_model.xml\" ).exists():\n",
    "        return\n",
    "    if remote_code:\n",
    "        # Phi-3のみ\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" --weight-format fp16\"\n",
    "    export_command += \" \" + str(fp16_model_dir)\n",
    "    # モデルのダウンロード開始時間\n",
    "    start_model_download = datetime.now()\n",
    "    print('export_command:', export_command)\n",
    "    os.system(export_command)  # モデルのダウンロード\n",
    "    # モデルのダウンロード終了時間\n",
    "    end_model_download = datetime.now() - start_model_download\n",
    "    print('export done', end_model_download.total_seconds())\n",
    "\n",
    "\n",
    "def convert_to_int8():\n",
    "    global export_command_base\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" --weight-format int8\"\n",
    "    export_command += \" \" + str(int8_model_dir)\n",
    "    # モデルのダウンロード開始時間\n",
    "    start_model_download = datetime.now()\n",
    "    print('export_command:', export_command)\n",
    "    os.system(export_command)  # モデルのダウンロード\n",
    "    # モデルのダウンロード終了時間\n",
    "    end_model_download = datetime.now() - start_model_download\n",
    "    print('export done', end_model_download.total_seconds())\n",
    "\n",
    "def convert_to_int4():\n",
    "    global export_command_base\n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    # 量子化の設定\n",
    "    model_compression_params  = compression_configs.get(model_name, compression_configs[\"default\"])\n",
    "    export_command = export_command_base + \" --weight-format int4\"\n",
    "    int4_compression_args = \" --group-size {} --ratio {}\".format(model_compression_params[\"group_size\"], model_compression_params[\"ratio\"])\n",
    "    if model_compression_params[\"sym\"]:\n",
    "        int4_compression_args += \" --sym\"\n",
    "    export_command += int4_compression_args + \" \" + str(int4_model_dir)\n",
    "    # モデルのダウンロード開始時間\n",
    "    start_model_download = datetime.now()\n",
    "    print('export_command:', export_command)\n",
    "    os.system(export_command)  # モデルのダウンロード\n",
    "    # モデルのダウンロード終了時間\n",
    "    end_model_download = datetime.now() - start_model_download\n",
    "    print('export done', end_model_download.total_seconds())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_command: optimum-cli export openvino --model microsoft/Phi-3-mini-4k-instruct --task text-generation-with-past --trust-remote-code --weight-format int4 --group-size 128 --ratio 0.8 ../../model/Phi-3-mini-4k-instruct/INT4\n",
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Framework not specified. Using pt to export the model.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using framework PyTorch: 2.3.0+cu121\n",
      "WARNING:root:Cannot apply model.to_bettertransformer because of the exception:\n",
      "The model type phi3 is not yet supported to be used with BetterTransformer. Feel free to open an issue at https://github.com/huggingface/optimum/issues if you would like this model type to be supported. Currently supported models are: dict_keys(['albert', 'bark', 'bart', 'bert', 'bert-generation', 'blenderbot', 'bloom', 'camembert', 'blip-2', 'clip', 'codegen', 'data2vec-text', 'deit', 'distilbert', 'electra', 'ernie', 'fsmt', 'gpt2', 'gptj', 'gpt_neo', 'gpt_neox', 'hubert', 'layoutlm', 'm2m_100', 'marian', 'markuplm', 'mbart', 'opt', 'pegasus', 'rembert', 'prophetnet', 'roberta', 'roc_bert', 'roformer', 'splinter', 'tapas', 't5', 'vilt', 'vit', 'vit_mae', 'vit_msn', 'wav2vec2', 'xlm-roberta', 'yolos']).. Usage model with stateful=True may be non-effective if model does not contain torch.functional.scaled_dot_product_attention\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/optimum/exporters/onnx/model_patcher.py:300: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/optimum/exporters/openvino/model_patcher.py:1006: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KMixed-Precision assignment \u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m128/128\u001b[0m • \u001b[36m0:01:14\u001b[0m • \u001b[36m0:00:00\u001b[0m00:02\u001b[0m00:06\u001b[0m\n",
      "\u001b[?25hINFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 25% (27 / 130)              │ 21% (25 / 128)                         │\n",
      "├────────────────┼─────────────────────────────┼────────────────────────���───────────────┤\n",
      "│              4 │ 75% (103 / 130)             │ 79% (103 / 128)                        │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n",
      "\u001b[2KApplying Weight Compression \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[35m 36%\u001b[0m \u001b[36m47/130\u001b[0m • \u001b[36m0:00:25\u001b[0m • \u001b[36m0:00:37\u001b[0mexport done 194.37537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Killed\n"
     ]
    }
   ],
   "source": [
    "convert_to_int4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_command: optimum-cli export openvino --model google/gemma-2b-it --task text-generation-with-past --weight-format int8 ../../model/gemma-2b-it/INT8\n",
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:521: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  warnings.warn(\n",
      "Framework not specified. Using pt to export the model.\n",
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using framework PyTorch: 2.3.0+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:1002: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/nncf/torch/dynamic_graph/wrappers.py:80: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  op1 = operator(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 100% (127 / 127)            │ 100% (127 / 127)                       │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━���━━━━━━━━━━━━━━━┙\n",
      "\u001b[2KApplying Weight Compression \u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m127/127\u001b[0m • \u001b[36m0:00:26\u001b[0m • \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hexport done\n"
     ]
    }
   ],
   "source": [
    "convert_to_int8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_command: optimum-cli export openvino --model microsoft/Phi-3-mini-4k-instruct --task text-generation-with-past --trust-remote-code --weight-format fp16 ../../model/Phi-3-mini-4k-instruct/FP16\n",
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Framework not specified. Using pt to export the model.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export done 15.817334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Killed\n"
     ]
    }
   ],
   "source": [
    "convert_to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of FP16 model is 7288.13 MB\n",
      "Size of model with INT4 compressed weights is 2336.74 MB\n",
      "Compression rate for INT4 model: 3.119\n"
     ]
    }
   ],
   "source": [
    "# モデルが保存されているディレクトリのサイズを確認\n",
    "\n",
    "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "if fp16_weights.exists():\n",
    "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    if compressed_weights.exists() and fp16_weights.exists():\n",
    "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバイスの選択\n",
    "support_devices = core.available_devices\n",
    "device = 'NPU' if 'NPU' in support_devices else 'CPU'  # 実機のNPUが使えればいいのだけれど。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPU'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61a9b1377a6423994fc0675686a4057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to run:', options=('INT4', 'FP16'), value='INT4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "available_models = []\n",
    "if int4_model_dir.exists():\n",
    "    available_models.append(\"INT4\")\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "if fp16_model_dir.exists():\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INT4'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_run.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../../model/Phi-3-mini-4k-instruct/INT4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korryu/Documents/TECH.C/Taltner/server/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "\n",
    "if model_to_run.value == \"INT4\":  # 4bitモデルを使う場合\n",
    "    model_dir = int4_model_dir\n",
    "elif model_to_run.value == \"INT8\":  # 8bitモデルを使う場合\n",
    "    model_dir = int8_model_dir\n",
    "else:\n",
    "    model_dir = fp16_model_dir  # 16bitモデルを使う場合\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "ov_model = OVModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device=device,\n",
    "    ov_config=ov_config,\n",
    "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 指示\n",
      "あなたは、ユーザーが初対面の相手との会話を補助するためのアシスタントです。\n",
      "ユーザーと相手の会話を円滑に進めるために、ユーザーの補佐をします。\n",
      "以下の要素を考慮して、4つの話題をJSONフォーマットで提示しなさい。\n",
      "\n",
      "# 条件\n",
      "1. 初対面の相手との会話を始めるのに適した話題の提供\n",
      "2. 会話の流れを考慮して、次の話題を提示\n",
      "3. 単語くらいの短い文章で話題を提示\n",
      "4. 出力フォーマットは{{JSON}}形式\n",
      "\n",
      "以下のフォーマットで出力してください：\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"話題1\",\n",
      "    \"話題2\",\n",
      "    \"話題3\",\n",
      "    \"話題4\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# 例1\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"好きな食べ物\",\n",
      "    \"どこ出身\",\n",
      "    \"好きなゲーム\",\n",
      "    \"最近見た映画\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# 例2\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"趣味\",\n",
      "    \"休日の過ごし方\",\n",
      "    \"好きな音楽\",\n",
      "    \"行ってみたい場所\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# 例3\n",
      "```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"好きなスポーツ\",\n",
      "    \"好きなアニメ\",\n",
      "    \"好きな漫画\",\n",
      "    \"好きな映画\"\n",
      "  ]\n",
      "}\n",
      "``` 自分: 「初めまして〜。俺AI専攻の人と話すの初めてなんだけど”AI専攻ってどんなことしてるの”？」\n",
      "\n",
      "相手: 「AI専攻は機械学習とか画像や音声認識とかLLMについて勉強してるよ」\n",
      " ```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"AIにおける最近の進展\",\n",
      "    \"機械学習の基本的な概念\",\n",
      "\n",
      "    \"画像認識の応用例\",\n",
      "\n",
      "    \"音声認識の革新的な進展\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# モデルの動作確認\n",
    "tokenizer_kwargs = LLM_MODELS_CONFIG[model_name].get(\"tokenizer_kwargs\", {})\n",
    "\n",
    "SYSTEM_PROMPT = '''\\\n",
    "# 指示\n",
    "あなたは、ユーザーが初対面の相手との会話を補助するためのアシスタントです。\n",
    "ユーザーと相手の会話を円滑に進めるために、ユーザーの補佐をします。\n",
    "以下の要素を考慮して、4つの話題をJSONフォーマットで提示しなさい。\n",
    "\n",
    "# 条件\n",
    "1. 初対面の相手との会話を始めるのに適した話題の提供\n",
    "2. 会話の流れを考慮して、次の話題を提示\n",
    "3. 単語くらいの短い文章で話題を提示\n",
    "4. 出力フォーマットは{{JSON}}形式\n",
    "\n",
    "以下のフォーマットで出力してください：\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"話題1\",\n",
    "    \"話題2\",\n",
    "    \"話題3\",\n",
    "    \"話題4\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# 例1\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"好きな食べ物\",\n",
    "    \"どこ出身\",\n",
    "    \"好きなゲーム\",\n",
    "    \"最近見た映画\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# 例2\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"趣味\",\n",
    "    \"休日の過ごし方\",\n",
    "    \"好きな音楽\",\n",
    "    \"行ってみたい場所\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# 例3\n",
    "```json\n",
    "{\n",
    "  \"topics\": [\n",
    "    \"好きなスポーツ\",\n",
    "    \"好きなアニメ\",\n",
    "    \"好きな漫画\",\n",
    "    \"好きな映画\"\n",
    "  ]\n",
    "}\n",
    "```\\\n",
    "'''\n",
    "\n",
    "conversation = '''\\\n",
    "自分: 「初めまして〜。俺AI専攻の人と話すの初めてなんだけど”AI専攻ってどんなことしてるの”？」\n",
    "\n",
    "相手: 「AI専攻は機械学習とか画像や音声認識とかLLMについて勉強してるよ」\n",
    "'''\n",
    "\n",
    "start_message: str = LLM_MODELS_CONFIG[model_name]['start_message']\n",
    "prompt_template: str = LLM_MODELS_CONFIG[model_name]['prompt_template']\n",
    "\n",
    "sys_prompt = start_message.format(\n",
    "    SYSTEM_PROMPT=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "prompt = sys_prompt + prompt_template.format(\n",
    "    user=conversation\n",
    ")\n",
    "\n",
    "input_tokens = tok(prompt, return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "\n",
    "answer = ov_model.generate(\n",
    "    **input_tokens,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "ans = tok.batch_decode(answer, skip_special_tokens=True)[0]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
